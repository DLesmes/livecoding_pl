import os
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_google_genai import ChatGoogleGenerativeAI
from app.settings import get_settings # Assuming settings.py is in the same 'app' directory

# Initialize settings and LLM
settings = get_settings()

# It's good practice to check for the API key early
if not settings.google_api_key:
    print("Critical Error: GOOGLE_API_KEY not found. Please ensure it's in your .env file and loaded correctly.")
    # Depending on the application structure, you might raise an exception here or handle it differently.
    # For a script, printing and exiting might be acceptable.
    # For now, we let the LLM initialization fail if the key is missing.
    
llm = ChatGoogleGenerativeAI(
    model="gemini-1.5-flash-latest", # Good for text summarization as well
    google_api_key=settings.google_api_key,
    temperature=0.3 # Slightly lower temperature for more factual summary
)

def summarize_transcript(transcript_file_path: str, output_summary_file_path: str) -> bool:
    """
    Reads a transcript from a file, sends it to Google Gemini for summarization,
    and saves the summary to a new text file.

    Args:
        transcript_file_path: Path to the text file containing the transcript.
        output_summary_file_path: Path to save the generated summary.

    Returns:
        True if successful, False otherwise.
    """
    if not settings.google_api_key:
        print("Error: GOOGLE_API_KEY not found. Cannot proceed with summarization.")
        return False

    try:
        print(f"Reading transcript from {transcript_file_path}...")
        with open(transcript_file_path, "r", encoding="utf-8") as f:
            transcript_text = f.read()
        
        if not transcript_text.strip():
            print("Error: Transcript file is empty or contains only whitespace.")
            return False
        print("Transcript loaded.")

    except FileNotFoundError:
        print(f"Error: Transcript file not found at {transcript_file_path}")
        return False
    except Exception as e:
        print(f"Error reading transcript file: {e}")
        return False

    print("Sending transcript to Gemini for summarization...")
    try:
        # Constructing the messages for the chat model
        messages = [
            SystemMessage(content="You are a helpful assistant that summarizes text. "
                                  "Provide a concise summary of the following video transcript. "
                                  "Focus on the main topics and key information presented."),
            HumanMessage(content=transcript_text),
        ]
        
        ai_response = llm.invoke(messages)
        summary = ai_response.content
        
        if not summary.strip():
            print("Warning: Gemini returned an empty summary.")
            # Optionally, still save the empty file or handle as an error
            # For now, we'll save it.
            
        print("Summary received.")

        with open(output_summary_file_path, "w", encoding="utf-8") as f:
            f.write(summary)
        print(f"Summary saved to {output_summary_file_path}")
        return True

    except Exception as e:
        print(f"Error during summarization or saving: {e}")
        return False

if __name__ == '__main__':

    input_transcript_file = "video_transcript.txt" # Generated by video_processor.py
    output_summary_file = "video_summary.txt"

    # Check if the transcript file exists before attempting to summarize
    if not os.path.exists(input_transcript_file):
        print(f"Prerequisite Error: Transcript file '{input_transcript_file}' not found.")
        print("Please run the video transcription process first.")
    else:
        success = summarize_transcript(input_transcript_file, output_summary_file)
        if success:
            print(f"Summarization process completed. Summary saved to '{output_summary_file}'.")
        else:
            print("Summarization process failed.")
